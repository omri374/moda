from moda.evaluators.metrics import (
    get_metrics_for_all_categories,
    _join_metrics,
    _initialize_metrics,
    get_final_metrics,
)

from sklearn.model_selection import TimeSeriesSplit


def eval_models(
    X,
    y,
    models,
    label_col_name="label",
    prediction_col_name="prediction",
    value_col_name="value",
    verbose=False,
    window_size_for_metrics=3,
    train_percent=70,
):
    """Evalutes one or more modeling with the provided datasets

     Parameters
     ----------
     :param X : pandas.DataFrame
         A pandas DataFrame with a two-leveled multi-index, the first
            indexing time and the second indexing class/topic frequency
            per-window, and a single column of a numeric dtype, giving said
            frequency.
    :param y : pandas.DataFrame
        A pandas DataFrame with a two-leveled multi-index, the first
            indexing time and the second indexing class/topic frequency
            per-window, and a single column of a integer type (-1,0,1), with the ground truth labels for each time and class/topic
    :param models : list of models to evaluate. Models should have a scikit-learn style fit & predict methods
    :param label_col_name : The name of the column holding the labeled data
    :param prediction_col_name : The name of the column generated by the model for prediction
    :param value_col_name : The name of the column holding the time series values
    :param window_size_for_metrics: shift size for searching for label/prediction hit

    Returns
    -------
    res : a dictionary of raw_metrics per model
    """

    if X is None:
        raise TypeError
    if y is None:
        raise TypeError

    per_model_res = {}
    date_time_index = X.index.levels[0]
    num_dates = len(date_time_index)

    train = date_time_index[: int(num_dates * train_percent / 100)]
    test = date_time_index[int(num_dates * train_percent / 100) :]

    for model in models:
        counter = 0

        if verbose:
            print("Model: {}".format(str(model)))

        X_train = _slice_set(X, train)
        y_train = _slice_set(y, train)

        X_test = _slice_set(X, test)
        y_test = _slice_set(y, test)

        metrics = eval_one_model(
            X_train,
            y_train,
            X_test,
            y_test,
            model,
            window_size_for_metrics,
            label_col_name,
            prediction_col_name,
            value_col_name,
            return_final_metrics=True,
        )
        counter += 1
        per_model_res[str(model.__name__)] = metrics

    return per_model_res


def eval_models_CV(
    X,
    y,
    models,
    label_col_name="label",
    prediction_col_name="prediction",
    value_col_name="value",
    n_splits=None,
    verbose=True,
    window_size_for_metrics=3,
):
    """Evalutes one or more modeling with the provided datasets, using time series cross validation

    Parameters
    ----------
    :param X : pandas.DataFrame
         A pandas DataFrame with a two-leveled multi-index, the first
            indexing time and the second indexing class/topic frequency
            per-window, and a single column of a numeric dtype, giving said
            frequency.
    :param y : pandas.DataFrame
        A pandas DataFrame with a two-leveled multi-index, the first
            indexing time and the second indexing class/topic frequency
            per-window, and a single column of a integer type (-1,0,1), with the ground truth labels for each time and class/topic
    :param models : list of models to evaluate. Models should have a scikit-learn style fit & predict methods
    :param label_col_name : The name of the column holding the labeled data
    :param prediction_col_name : The name of the column generated by the model for prediction
    :param value_col_name : The name of the column holding the time series values
    :param window_size_for_metrics: shift size for searching for label/prediction hit
    :param n_splits : integer
    The number of splits for TimeSeriesSplit. see  http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html

    Returns
    -------
    res : a dictionary of metrics per model
    """

    if X is None:
        raise TypeError
    if y is None:
        raise TypeError

    if n_splits is None:
        n_splits = int(len(X.index.levels[0]) / 2)
        print("Running {} splits".format(n_splits))

    if n_splits > len(X.index.levels[0]):
        n_splits = int(len(X.index.levels[0]) / 2)
        print(
            "Warning: n_splits cannot be larger than the number of dates. Reducing n_splits to {}".format(
                n_splits
            )
        )

    per_model_res = {}

    datetimeindex = X.index.levels[0]
    for model in models:
        counter = 0

        tscv = TimeSeriesSplit(n_splits=n_splits)
        if verbose:
            print("Model: {}".format(str(model)))

        categories = X.index.levels[1]
        prev_metrics = _initialize_metrics(categories)

        for train, test in tscv.split(datetimeindex):
            if verbose:
                print(
                    "Iteration: %s, Train size: %s, Test size: %s, Data size: %s "
                    % (counter, len(train), len(test), len(datetimeindex))
                )

            X_train = _slice_set(X, datetimeindex[train])
            y_train = _slice_set(y, datetimeindex[train])

            X_test = _slice_set(X, datetimeindex[test])
            y_test = _slice_set(y, datetimeindex[test])

            metrics = eval_one_model(
                X_train,
                y_train,
                X_test,
                y_test,
                model,
                window_size_for_metrics,
                label_col_name,
                prediction_col_name,
                value_col_name,
                return_final_metrics=False,
            )
            metrics = _join_metrics(metrics, prev_metrics)

        final_metrics = get_final_metrics(metrics, summarized=True)
        if verbose:
            print(final_metrics)
        per_model_res[str(model.__name__)] = final_metrics
    return per_model_res


def eval_one_model(
    X_train,
    y_train,
    X_test,
    y_test,
    model,
    window_size_for_metrics,
    label_col_name="label",
    prediction_col_name="prediction",
    value_col_name="value",
    return_final_metrics=True,
):
    """
    Returns metrics for one model and one pair of train/test sets.
    :param X_train: training set DataFrame
    :param y_train: training label DataFrame
    :param X_test: testing set DataFrame
    :param y_test: testing label DataFrame
    :param model: Model to be evaluated
    :param window_size_for_metrics: shift size for searching for label/prediction hit
    :param label_col_name: name of label column in test
    :param prediction_col_name: label of prediction column that the model creates (
    :param value_col_name: Name of value column containing the time series values
    :return:
    """
    if (y_test is not None) and (len(y_test) > 0):
        # run the model
        print("Fitting...")
        model.fit(X=X_train, y=y_train)
        print("Predicting...")
        prediction = model.predict(X=X_test)

        # evaluate results, aggregate raw_metrics
        raw_metrics = get_metrics_for_all_categories(
            X_test,
            prediction,
            y_test,
            value_col_name=value_col_name,
            label_col_name=label_col_name,
            prediction_col_name=prediction_col_name,
            window_size_for_metrics=window_size_for_metrics,
        )

    if return_final_metrics:
        final_metrics = get_final_metrics(raw_metrics, summarized=False)
    else:
        final_metrics = raw_metrics
    return final_metrics


def _slice_set(X, dates):
    """
    Returns a copy of the original data, sliced based on the specified dates
    :param X: dataset with a MultiIndex consisting of date as pd.DatetimeIndex and category
    :param dates: list of dates for filtering
    :return: subset of X (copy)
    """
    new_samples = X.copy().loc[dates]
    new_samples.index = new_samples.index.remove_unused_levels()
    return new_samples
